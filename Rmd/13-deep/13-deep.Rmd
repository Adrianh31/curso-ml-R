---
title: "Aprendizaje profundo"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Alberto Torres y Víctor Gallego"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"

---



class: middle, center, inverse

# Introducción

---


## Repaso de Deep Learning

* Modelo principal: red profunda (**feed-forward**): composición de **proyecciones lineales** y **no-linealidades**

\begin{align}
h^{(i+1)} &= Wz^{(i)} + b   \\
z^{(i+1)} &= \sigma (h^{(i+1)}) \\
\end{align}

* Al final: añadir coste apropiado para regresión o clasificación.

* Las redes neuronales se conocen desde mediados del siglo XX, pero su fuerte resurgimiento no fue hasta esta década:

  * Paralelización en tarjetas gráficas (**GPUs**).
  
  * Librerías de **diferenciación automática**.

---

## Diferenciación Automática (AD) (1)

* ¿Cómo calcular el gradiente en una red profunda?

--

* **A mano**: no escala a nuevas arquitecturas, propenso a errores.

* **Diferenciación numérica**: acumulación de errores y elevado coste computacional.

\begin{equation}
\frac{\partial E_n}{\partial w_{ji}} = \frac{E_n(w_{ji} + \epsilon) - E_n(w_{ji} - \epsilon)}{2 \epsilon} + O(\epsilon^2)
\end{equation}

* **Diferenciación simbólica**: manipulación exacta de expresiones (mediante tablas de derivadas), pero explosión en la cantidad de términos:

<center>
![:scale 50%](./img/ad1.png)
</center>





---

## Diferenciación Automática (AD) (2)

* Surge la **diferenciación automática o algorítmica**: aplica diferenciación simbólica pero solo a expresiones simples, y al componerlas, actualiza los resultados numéricos parciales (que serán **exactos**)

* Ejemplo para calcular la derivada de $y = f(x_1, x_2) = \log (x_1) + x_1 x_2 - \sin (x_2)$ en $(x_1, x_2) = (2, 5)$:

<center>
![:scale 100%](./img/ad2.png)
</center>



---

## Diferenciación Automática (AD) (3)

* ¿Por qué **backpropagación**?

* Ejemplo: considera una serie de funciones $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$,  $g : \mathbb{R}^m \rightarrow \mathbb{R}^m$ y $h : \mathbb{R}^m \rightarrow \mathbb{R}$. Queremos obtener la derivada de su composición, $\frac{\partial (h \circ g \circ f)}{\partial x}$


* Queda que 

\begin{equation}
\frac{\partial (h \circ g \circ f)}{\partial x} = \frac{\partial h}{\partial g} \frac{\partial g}{\partial f} \frac{\partial f}{\partial x}
\end{equation}

* Asociando $\frac{\partial h}{\partial g} ( \frac{\partial g}{\partial f} \frac{\partial f}{\partial x} )$, queda un producto matriz-matriz y otro producto vector-matriz.

* Asociando $( \frac{\partial h}{\partial g} \frac{\partial g}{\partial f} )\frac{\partial f}{\partial x}$, ¡solo hay que hacer productos vector-matriz!: mucho más eficiente.

--

* En ML es habitual optimizar funciones de tipo $\mathbb{R}^d \rightarrow \mathbb{R}$, por tanto es más eficiente propagar los gradientes hacia atrás (**backpropagation**) que hacia adelante (**forward propagation**).


---


## Optimizando mediante SGD o Adam.

* Una vez hemos calculado el gradiente en un punto mediante AD, las opciones actuales más populares son

* **Descenso por el gradiente estocástico** (SGD): ya visto, estimación usando mini-batches.

* **Adam** (2014: https://arxiv.org/abs/1412.6980) : corrige el gradiente estimando una ventana móvil de su media y de su varianza.


<center>
![:scale 80%](./img/adam.png)
</center>


---

## Volviendo a Deep Learning


* Ya tenemos todos los ingredientes:

  * Datasets enormes.

  * Redes neuronales como **aproximadores universales**.
  
  * Librerías para **diferenciación automática**: tensorflow, keras, pytorch...
  
  * Potentes CPUs o GPUs para **paralelizar a lo largo de cada ejemplo del mini-batch**.
  
* ¿Qué falta en muchas ocasiones?

--

  * Solo teóricamente las redes neuronales son totalmente expresivas.
  
  * Conviene añadir un **sesgo inductivo** (**inductive bias**) para ayudar al aprendizaje:
  
      * Imágenes, señales: invarianza a traslaciones, escala: **redes convolucionales**.
    
      * Texto, secuencias: sensibilidad al orden de los símbolos: **redes recurrentes**.

---



class: middle, center, inverse

# Redes recurrentes (RNNs)

---

## Intuición

* Las redes neuronales recurrentes (*recurrent neural networks*, RNNs) surgen de la necesidad de **procesar secuencias** de datos (fundamentalmente textos).

* ¿Qué hacemos cuando los inputs pueden tener diferentes longitudes?

* Imagen


---

## Esquema original

* Describir la de Elman


---

## Ejemplo


---

## Grafo computacional y backpropagación truncada



---

## Problema de la RNN original

* Explosión y desvanecimiento de gradientes


---

## Conexiones residuales


---

## Gated Recurrent Unit (GRU)

---

## Long-Short Term Memory network (LSTM)



---




---

## Resumen

* Las RNNs permiten gran flexibilidad en el diseño de la arquitectura.

* Las RNNs originales son simples pero no funcionan bien.

* Más común: utilizar **LSTM** o **GRU** para "mejorar" el gradiente.

* El flujo de gradiente hacia atrás puede explotar o desvanecerse en las RNNs: la **explosión** se controla acotando el gradiente (clipping). El **desvanecimiento** mediante conexiones aditivas (LSTM, GRU).

* Las búsqueda de arquitecturas más simples es área de investigación actual.

* Todavía hay escasos avances teóricos, se necesita más investigación.


---


class: middle, center, inverse

# Aplicación de RNNs a Procesamiento de Lenguaje Natural

---

## Cambio de paradigma

* Pre 2000s: **simbólico, basado en reglas**

  * Lenguaje entendido como conjunto de elementos y reglas para combinarlos.
  * Gramáticas independientes de contexto (Chomsky).
  * Más adecuado a lenguajes artificiales (de programación) que naturales (humanos).
  
<center>
![:scale 30%](./img/nlp1.png)
</center>
  
* Después: **estadístico, basado en datos**

  * Lenguaje entendido como probabilidades de secuencias de palabras.
  * Cálculo de frecuencias de palabras, n-gramas, etc.
  * Más adecuado a lenguajes naturales que artificiales.
  * Combinación con modelos profundos: **estado del arte**.

---

## De n-gramas a word embeddings (1)

* **Bag of words**: contamos la aparición (o frecuencia) de cada palabra: <span style="color:blue">El atento alumno $\rightarrow$ (El), (atento), (alumno)</span>.

* Representaríamos la frase como 

$$
\left(0 , \ldots, 1, 0, \ldots, 0, 1, 0, \ldots, 1, 0 \right) \in \lbrace 0, 1 \rbrace^{|V|}
$$

* donde $|V|$ es el número de palabras de nuestro vocabulario $V$.

* Problema: no tiene en cuenta el orden (y contexto) de las palabras. Solución (parcial):

* **2-gramas**: contamos ahora pares consecutivos de palabras: <span style="color:blue">(El, atento), (atento, alumno)</span>.

* Ahora la representación es sobre $\lbrace 0, 1 \rbrace^{|V|^2}$.

--

* **n-gramas**: explosión combinatoria...

* Ha sido lo estándar hasta $\sim 2013$. ¿Podemos encontrar una representación más compacta?


---

## De n-gramas a word embeddings (2)


* Cada palabra (representada mediante OHE) se mapea a un espacio continuo: $\lbrace 0, 1 \rbrace^{|V|} \rightarrow \mathbb{R}^m$.

* Mediante una transformación lineal $z_i = E w_i$ donde $E$ es una matriz de tamaño $m \times |V|$. Típicamente $m = 300 << |V|$.

<center>
![:scale 60%](./img/wordemb.png)
</center>

* **Combaten la catástrofe de la dimensionalidad**, mediante una compresión de los datos, pasando de un espacio discreto a uno continuo.

* Al proyectar a un espacio continuo, esperamos que palabras parecidas (sinónimos) se encuentren cerca (bajo la métrica euclidea).

---


## Álgebra lineal en el espacio de palabras (1)

* **One-hot encoding**: no hay noción de vecindad entre palabras, cualquier palabra está igual de lejos que las demás.

* **Word embeddings** (codificación densa): podemos usar la distancia euclídea (u otras) en $\mathbb{R}^m$.

<center>
![:scale 32%](./img/wordemb2.png)
</center>

---

## Álgebra lineal en el espacio de palabras (2)

* Como estamos en un espacio vectorial ( $\mathbb{R}^m$ ), podemos realizar operaciones con vectores (word embeddings).

* Aprenden ciertas analogías entre palabras.

<center>
![:scale 60%](./img/wordemb3.png)
</center>

---

## word2vec (2013)

* La pregunta del millón: **¿cómo obtener la matriz $E$ de word embeddings?**

--

* Basado en la **hipótesis distribucional** del lenguaje (J. Firth 1957): el significado de una palabra puede inferirse a partir del contexto (palabras vecinas en las que aparece)

* El modelo word2vec presenta dos variantes:

  * **CBoW**: dado un contexto, predecir palabra central.
  
  * **Skip-gram**: dada la palabra central, predecir el contexto.

<center>
![:scale 60%](./img/wordemb4.jpg)
</center>


---


## Uso de embeddings preentrenados

* Aunque los embeddings pueden inicializarse aleatoriamente (como los pesos de una red neuronal estándar) y aprenderse durante la tarea,

* Una técnica habitual es cargar unos **word embeddings preentrenados**, para ahorrar tiempo y datos.

* Una vez ya tenemos los embeddings, se los acoplamos a cualquier modelo (regresión logística, red neuronal) y procedemos con el entrenamiento.


* https://fasttext.cc/ mejora de word2vec (contiene información de prefijos y sufijos).

* https://fasttext.cc/docs/en/crawl-vectors.html en castellano, entrenados sobre los artículos de la Wikipedia y CommonCrawl.


---

## Clasificación de textos

---

## Generación de textos

* https://openai.com/blog/better-language-models/

* GPT-2 es un modelo de lengauje entrenado sobre un corpus de 40GB de datos (8 millones de páginas webs).

* La versión grande del modelo consta de 1500 millones de parámetros.

* Generación de historias online en https://talktotransformer.com/


---

## Traducción automática


---

## Subtitulación de imágenes

<center>
![:scale 80%](./img/caption.png)
</center>

* Explain Images with Multimodal Recurrent Neural Networks: https://arxiv.org/pdf/1410.1090.pdf

* Show and Tell: A Neural Image Caption Generator: https://arxiv.org/pdf/1411.4555.pdf





---


class: middle, center, inverse

# Redes convolucionales

---



class: middle, center, inverse

# Recursos adicionales

---

## Enlaces de interés

* https://reddit.com/r/LearnMachineLearning: nivel introductorio/medio.

* https://reddit.com/r/machinelearning: discusiones sobre artículos y temas de actualidad.

* https://medium.com/topic/machine-learning: artículos hacia audiencia general.

* ???




