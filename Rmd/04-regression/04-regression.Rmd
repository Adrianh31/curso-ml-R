---
title: "Modelos lineales: regresión"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Alberto Torres Barrán"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    includes:
      before_body: mathjax.html
  keep_md: true
---

## Modelos lineales

Variables provienen de múltiples fuentes:
* variables quantitativas
* transformaciones (logaritmo, raiz cuadrada, etc.)
* expansiones de base, $x_2 = x_2^2$
* variables *dummy*
* interacciones entre variables, $x_3 = x_1 \times x_2$

Pero siempre: modelo lineal en los parámetros

---

## Regresión lineal y logística

La salida $y$ es continua, $y \in \Rbb$

* Regresión lineal (*MSE* o *RSS*) $$\min_w\, ||y - \Xbf w||_2^2$$

--

La salida $y$ es discreta, $y \in \{ 0, 1 \}$

* Regresión logística (*log-loss*) $$\min_w\, -(y^T \log[\sigma(\Xbf w)] + (1 - y)^T \log[1 - \sigma(\Xbf w)])$$

???

La función de pérdida log-loss es un poco distinta a como la vimos antes
  * Tiene un menos delante, porque queremos minimizar y no maximizar la verosimilitud
  * Propiedad de la función sigmoidea $\sigma(-x) = 1 - \sigma(x)$

---

## Generalized linear models (GLM)

* Generalización de la regresión lineal que permite distribuciones de errores distintas de la distribución normal.

* Componentes:
  
  * Distribución de $Y_i$ con media $\mu_i$
  * Predictor lineal, $$g(\mu_i) = w^T x_i$$ donde $g(\cdot)$ es la función de media

* La función de media proporciona la relación entre la media de la distribución y el predictor lineal

* El inverso de la función de media, $g^{-1}(\cdot)$ se conoce con el nombre de **función de enlace**

---

##Ejemplo: distribución binomial

* La regresión logística es un caso particular de GLM donde la distribución de $Y$ es la binomial

* La función de media es la logística,

$$\mu = g^{-1}(w^T x_i) = \frac{1}{1 + \exp(-w^T x_i)}$$

* La función de enlace es la inversa de la anterior,

$$w^T x_i = g(\mu) = \ln\left(\frac{\mu}{1 - \mu}\right)$$

* Para cada distribución, hay una función de enlace "canónica" que es la que se usa habitualmente

---

## Ejemplo: distribución de Poisson

* Esta distribución está indicada cuando queremos modelizar una variable de salida entera y no real (por ej. conteos)

* Función de media

$$\mu = \exp(w^T x_i)$$

* Función de enlace

$$w^T x_i = \ln(\mu)$$

* Otras distribuciones posibles son la Gamma, Exponencial, Multinomial, etc.

---

## GLMs en R

* La función para ajustar modelos lineales generalizados es `glm()`

* Tiene los mismos argumentos principales que `lm()`, pero además tenemos que especificar la distribución de la variables dependiente con el parámetro `family`

* Por defecto se usa la función de enlace "canónica", pero esto se puede modificar (ver ayuda)

* Implementa el algoritmo IRLS (Newton-Raphson), que se puede generalizar para cualquier GLM donde la distribución pertenece a la familia exponencial

Ejemplo: regresión logística

```{r message=FALSE, warning=FALSE}
library(MASS)
fit <- glm(type ~ ., data=Pima.tr, family=binomial)
```

---

## Problemas de mínimos cuadrados

1. Calidad de predicción: 
  
  * poco sesgo pero potencialmente mucha varianza
  
  * podemos mejorar las predicciones reduciendo el valor de algunos coeficientes
  
  * aumenta ligeramente el sesgo pero disminuye mucho la varianza

2. Interpretación: 

  * el valor de los coeficientes nos da una idea de las variables mas relevantes
  
  * nos gustaria encontrar un subconjunto de los mejores

???

Teorema de Gauss-Markov

El estimador de minimos cuadrados es BLUE

---

## Regularización

* Regresión *ridge* (*MSE* + regularización $l_2$): $$\min_w\, ||y - \Xbf w||_2^2 + ||w||_2^2$$

--
  
* ¿Regresión logística ridge?

--

* ¿Otras funciones de regularización?

---

class: middle, center, inverse

# Métodos de seleccion

---

## Regresión *best subset*

* Mantenemos solo un subconjunto de las variables y eliminamos el resto del modelo

* Para $k \in \{0, 1, 2, \dots, d\}$ se resuelve $$\min_w\, || y - \Xbf w ||_2^2 \quad \mbox{s.t.}\; ||w||_0 \leq k$$
donde $||w||_0 = \sum_{i=1}^d{\mathbb{I}(w_i \neq 0)}$

* $\mathbb{I}(\cdot)$ es la función indicatriz (cuenta el número de elementos distintos de 0)

* La restricción hace que el problema sea NP-completo, $$C_{d,k}=\binom{d}{k} = \frac{d!}{k!(d-k)!}$$

* Algoritmos clásicos pueden resolver $d \approx 30$

* Avances recientes, [(Bertsimas et al., 2015)](https://arxiv.org/pdf/1507.03133.pdf): $d \in [100, 1000]$ 

???

$C_{30, 10} \approx 30M$

---

class: center, middle

![](./img/best_subset.png)

???

$C_{8, 2} = 28$

$C_{8, 3} = 56$

$C_{8, 4} = 70$

---

## Best subset en R

Paquete `leaps`

---

## Regresión *stepwise*

* *Forward-Stepwise*:
  
  1. Añadir al modelo la variable que proporciona mejor ajuste
  2. Repetir hasta añadir $k$ variables

* *Backward-Stepwise* 

  1. Empezar con las $d$ variables
  2. Eliminar iterativamente la menos relevante para el ajuste 

* Algoritmos avariciosos

* No buscan entre todas las posibles combinaciones de subconjuntos de tamaño $k$

* En cada paso solo se ajustan $d-k$ modelos

???

Mas sesgo pero potencialmente menos varianza, al imponer mas estructura
(no se busca el mejor)

---

## *Stepwise* en R

* Función `step`

* Procedimiento hibrido: se procede como *forward* pero en cada paso está la opción de eliminar alguna variable añadida previamente

* Añade o elimina variables en grupos (por ej. si son variables *dummy*)

* Selecciona automáticamente el valor óptimo de $k$

---

## Selección de k

* Secuencia de modelos indexada por $k$ (igual que *best subset*)

* Elegir $k$ como el que minimiza el error de validación cruzada 

* Validación cruzada: estimación del error de generalización o error *extra-sample*

* Error de entrenamiento es demasiado optimista (error *in-sample*)

* Alternativa: cuantificar el "optimismo" y minimizarlo (AIC, BIC y derivados)

???

ESL secciones 7.4 en adelante

---

class: inverse, middle, center

# Métodos de reducción

---

## Regresión *rigde*

ESL sección 3.4.1

---

## Lasso: motivación

* Métodos de selección: 
   
   * ![:colorText green](modelos interpretables)
    
   * ![:colorText red](proceso discreto, las variables están incluidas o no)


* Regresión *ridge*: 

  * ![:colorText green](proceso continuo, todos los coeficientes se reducen)

  * ![:colorText red](rara vez son exactanente 0, modelos no interpretables)


* Lasso es una técnica intermedia:

  * ![:colorText green](reduce algunos coeficientes)
  
  * ![:colorText green](pone el resto a 0)

???

Modelos seleccion: variables entran o salen del modelo

---

## Lasso: formulación

* Problema optimización: $$\min_w\, || y - \Xbf w ||_2^2\quad \text{s.t. }\; ||w||_1 \leq t$$

* Equivalente: $$\min_w\, || y - \Xbf w ||_2^2 + \lambda ||w||_1$$

* $\lambda$ o $t$ son hiper-parámetros

  * $\uparrow \lambda$ o $\downarrow t$, se reducen los coeficientes (más regularización)

  * $\downarrow \lambda$ o $\uparrow t$, aumentan los coeficientes (menos regularización)
  
* $t$ suficientemente pequeño (o $\lambda$ suficientemente grande), algunos coeficientes = 0

---

class: middle, center

![:video 600 600](./img/lasso.mp4)


---

class: middle, center

![](./img/orthonormal.png)

???

$X^T X = I$, no ocurre en la practica pero interesante para ver la diferencia entre los estimadores

---

class: middle, center

![](./img/constraint_regions.png)

---

* Comparación Lasso, *best subset* y *forward stepwise* [(Hastie et al., 2017)](https://arxiv.org/pdf/1707.08692.pdf)

* Experimentos en [Github, best-subset](https://github.com/ryantibs/best-subset/)

* [*Relaxed* Lasso](https://stat.ethz.ch/~nicolai/relaxo.pdf)

???

[Relaxed Lasso](https://stats.stackexchange.com/questions/122955/why-is-relaxed-lasso-different-from-standard-lasso)

---

## Lasso: limitaciones

1. Si $d > n$, como mucho $n$ coeficientes son $\neq 0$

    * Limitación desde el punto de vista de selección de variables

2. Variables con correlación alta $\Longrightarrow$ Lasso selecciona una "aleatoriamente"

3. Si $n > d$ y hay variables con correlación alta $\Longrightarrow$ error de Ridge < error de Lasso

---

## Otras penalizaciones

* Podemos generalizar Lasso, Ridge y Best subset como $$\min_w || y - \Xbf w ||_2^2 + \lambda || w ||_p^p$$
donde, $$|| w ||_p^p = \sum_{i=1}^d{|x_i|^p}$$

* $0 \leq p < 1$, no convexas

* $p = 1$, convexa y no diferenciable

* $p > 1$ convexas y diferenciables

---

## Interpretación Bayesiana

* Regularización = distribución a priori de los parámetros $w$

* Ridge regressión: distribución Normal

* Lasso: distribución de Laplace, $\tau = 1/\lambda$ $$f(w) = \frac{1}{2\tau} \exp\bigg(-\frac{|w|}{\tau}\bigg)$$

* Estimadores: máximo de la distribución a posteriori (MAP)
  
  * Ridge: coincide con la media
  * Lasso y Best subset: moda

---

class: center, middle

![](./img/laplace.svg)

[Wikipedia, "Laplace distribution"](https://en.wikipedia.org/wiki/Laplace_distribution)

---

class: middle, center

![](./img/penalties.png)

---

## Elastic Net

* Combina regularización Lasso y Ridge: $$\min_w\, || y - \Xbf w ||_2^2 + \lambda_1 || w ||_1 + \lambda_2 || w ||_2^2$$

* Otra parametrización con $\alpha \in (0, 1)$: $$\min_w\, || y - \Xbf w ||_2^2 + \lambda(\alpha || w ||_1 + (1 - \alpha) || w ||_2^2)$$

* Equivalentes con 

  * $\alpha = \lambda_1/(\lambda_1 + \lambda_2)$ 
  
  * $\lambda = \lambda_1 + \lambda_2$
  
* Selecciona variables y reduce el resto de coeficientes

---

class: center, middle

![:scale 85%](./img/penalties.svg)

???

Ventaja de parametrizacion 2: $\lambda$ te da la distancia al origen, y $\alpha$ la curvatura

---

## Notación

* Lasso (Elastic Net) suelen hacer referencia a: minimizar MSE + norma $l_1$ (+ norma $l_2$)

* MSE puede reemplazarse por otras funciones de pérdida

* Por ej. cualquier GLM

* En esos casos hablamos de regresión logística, Poisson, Gamma + regularización Ridge/Lasso/Elastic Net

---

## Elastic Net en R

Paquete `glmnet`

---

class: middle, center, inverse

# Métodos de reducción

---

## Principal Component Regression

---

## Partial Least Squares

---

## Algoritmos

 * FISTA

 * Descenso coordinado

---
 
## Variantes del Lasso

Group Lasso, Fused Lasso, Generalized Lasso, Relaxed Lasso