---
title: "Redes neuronales"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Alberto Torres Barrán y Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    includes:
      before_body: mathjax.html
  keep_md: true

---

## Perceptron

* Combinación lineal de las variables de entrada

* No linealidad: función de activación (por ejemplo la función escalón)

.center[
![](./img/perceptron.png)

[Fuente](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)
]
---

## Ejemplo perceptron

[
![](https://raw.githubusercontent.com/miku/nntour/master/gifs/perceptron-pla-14-steps.gif)](https://github.com/miku/nntour)  

---

## Problema XOR

* Observaciones no separables linealmente $\Rightarrow$ perceptrón no encuentra solución

.center[![](./img/xor.png)

[Fuente](https://www.freecodecamp.org/news/the-history-of-deep-learning-explored-through-6-code-snippets-d0a0e8545202/)
]
---

## Redes neuronales

* Añadimos una capa intermedia (capa oculta)

.center[
![:scale 45%](./img/Colored_neural_network.svg)

[Fuente](https://en.wikipedia.org/wiki/Artificial_neural_network)
]

---

## RN como aproximación de funciones

Teorema de aproximación universal

PERO: número de neuronas exponencialmente grande

---

## Aprender representaciones

* Modelos como las SVMs generan una nueva representación de los datos de entrada en un espacio ampliado

* Esperamos que en este nuevo espacio el problema de aprendizaje sea más sencillo

* Esta nueva representación también se puede generar de forma manual (*feature engineering*)

* Crear nuevas variables es, típicamente:

  1. critico para obtener buen rendimiento
  
  2. muy dependiente del problema
  
* Ejemplo: extraer variables de datos no tabulares (audio, video, imágenes, texto)

---

class: center, middle

![](./img/coordinate_change.png)

---

## RN vs otros modelos
  
* Automatizan la creación de nuevas variables

* Esto simplifica la resolución de nuevos problemas:
  
  1. no necesario tanto conocimiento específico 
  
  2. proceso mucho menos costoso que crear nuevas variables a mano
  
* Además la creación de estas nuevas representaciones forma parte del aprendizaje

  * específicas para la tarea a resolver $\Rightarrow$ mejor rendimiento
  
---

## ¿Por qué ahora?

* Redes convolucionales y *backpropagation* son de 1989

* Redes recurrentes como la LSTM de 1997

* Desde 2010 varios avances han contribuido al exíto de las redes neuronales:

  1. hardware
  
  2. datos
  
  3. avances algorítmicos
  
---

## Hardware

GPUs

---

## Datos

Desarrollo exponencial de la capacidad de almacenamiento

Internet: recolectar y distribuir conjuntos de datos de forma sencilla

  1. Wikipedia (texto)
  
  2. Youtube (video)
  
  3. Flickr (imágenes)
  
---

## Algoritmos


---

## Democratización

* Antes: programar en GPUs $\Rightarrow$ lenguajes específicos (CUDA) y C++

* Desde 2010: multiples librerias (Torch, Theano, Caffe, Tensorflow) que

  1. realizan diferenciación automática
  
  2. implementan tensores y operaciones con los mismos
  
  3. hacen uso de la GPU de forma transparente

* Ahora: varias librerías de alto nivel que implementan capas de aprendizaje profundo:

  1. **Keras**
  
  2. PyTorch
  
---

class: middle, center, inverse
# Feed-forward Neural Networks

---

## Introducción

* Hasta ahora hemos visto modelos de regresión y clasificación que recibían como input combinaciones lineales de *funciones base*.

* Para que estos modelos resulten prácticos, debemos adaptar las funciones base a los datos.

* Idea: 

  1. Fijar el número de funciones base de antemano
  
  2. Darles forma paramétrica
  
  3. Aprender parámetros usando los datos.

* **Aprender la representación**.

* *Perceptrón multicapa* o *feed-forward neural network*.

---

## Perceptrón multicapa

* ¿Cómo parametrizamos las funciones de base?

* Hasta ahora

\begin{equation}
y(x,w) = f \left( \sum_{j=1}^M w_j \phi_j(x) \right)
\end{equation}

* Siendo $f$ una **activación no lineal**.

  