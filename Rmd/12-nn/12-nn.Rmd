---
title: "Redes neuronales"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Alberto Torres Barrán y Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    includes:
      before_body: mathjax.html
  keep_md: true

---

---
class: middle, center, inverse

# Introducción

---

## Perceptron

* Combinación lineal de las variables de entrada

* No linealidad: función de activación (por ejemplo la función escalón)

.center[
![](./img/perceptron.png)

[Fuente](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)
]
---

## Ejemplo perceptron

[
![](https://raw.githubusercontent.com/miku/nntour/master/gifs/perceptron-pla-14-steps.gif)](https://github.com/miku/nntour)  

---

## Problema XOR

* Observaciones no separables linealmente $\Rightarrow$ perceptrón no encuentra solución

.center[![](./img/xor.png)

[Fuente](https://www.freecodecamp.org/news/the-history-of-deep-learning-explored-through-6-code-snippets-d0a0e8545202/)
]
---

## Redes neuronales

* Añadimos una capa intermedia (capa oculta)

.center[
![:scale 45%](./img/Colored_neural_network.svg)

[Fuente](https://en.wikipedia.org/wiki/Artificial_neural_network)
]

---

## Teorema de aproximación universal

* Asumimos:

  - red neuronal feed-forward
  
  - una capa oculta
  
  - número de neuronas finito
  
  - algunas funciones de activación (por ej. sigmoidea)
  
* Aproxima cualquier función continua con precisión arbitrarea

* **Pero**:el número de neuronas necesario es exponencialmente grande

---

## Aprender representaciones

* Modelos como las SVMs generan una nueva representación de los datos de entrada en un espacio ampliado

* Esperamos que en este nuevo espacio el problema de aprendizaje sea más sencillo

* Esta nueva representación también se puede generar de forma manual (*feature engineering*)

* Crear nuevas variables es, típicamente:

  1. critico para obtener buen rendimiento
  
  2. muy dependiente del problema
  
* Ejemplo: extraer variables de datos no tabulares (audio, video, imágenes, texto)

---

class: center, middle

![](./img/coordinate_change.png)

---

## RN vs otros modelos
  
* Automatizan la creación de nuevas variables

* Esto simplifica la resolución de nuevos problemas:
  
  1. no necesario tanto conocimiento específico 
  
  2. proceso mucho menos costoso que crear nuevas variables a mano
  
* Además la creación de estas nuevas representaciones forma parte del aprendizaje

  * específicas para la tarea a resolver $\Rightarrow$ mejor rendimiento
  
---

class: center, middle

![](img/nn_vs_rest.png)
  
---

## Ejemplo

* Clasificar imágenes médicas en sano/enfermo

* Antes: una parte importante del trabajo consistía en procesar las imágenes del microscopio para extraer características:

  1. segmentar células
  
  2. identificar núcleo
  
  3. etc.
  
* Redes neuronales profundas extraen automáticamente características **útiles para la tarea de clasificar**

.center[
![](img/nn_vs_rest_ex.png)
]

---

## ¿Por qué ahora?

* Redes convolucionales y *backpropagation* son de 1989

* Redes recurrentes como la LSTM de 1997

* Desde 2010 varios avances han contribuido al exíto de las redes neuronales:

  1. hardware
  
  2. datos
  
  3. avances algorítmicos
  
---

## Hardware

* Entre 1990 y 2010 las CPUs estándar incrementaron su velocidad un factor de 5000

* Modelos de redes neuronales entrenables en un portátil estándar

* No suficiente para modelos más complejos (recurrentes, convolucionales)

* GPUs: unidades de procesamiento gráfico

  1. procesadores más sencillos
  
  2. útiles para procesar grandes bloques de datos en paralelo
  
* Entrenar una red necesita muchas multiplicaciones de matrices

* NVIDIA Titan X (aprox. 1000$) tiene 350 veces más potencia que un portátil moderno $\Rightarrow$ 6.6 trillones de operaciones en coma flotante/segundo

---

## Datos

* Desarrollo exponencial de la capacidad de almacenamiento

* Internet: recolectar y distribuir conjuntos de datos de forma sencilla

  1. Wikipedia (texto)
  
  2. Youtube (video)
  
  3. Flickr (imágenes)
  
* Competiciones de benchmark, por ej. ImageNet o Kaggle

---

## Algoritmos

* Hasta 2010 no existían formas fiables de entrenar redes neuronales profundas (solo 1 o 2 capas)

  1. se puede calcular el gradiente (*backpropagation*)

  2. la señal del error se desvanece en las capas intermedias

* Ciertos avances algorítmicos aliviaron el problema:

  1. mejores funciones de activación
  
  2. mejor inicialización de los pesos

  3. mejores algoritmos de optimización (variantes de SGD)
  
* Se pueden entrenar redes con 10 o más capas

* Hoy en día otros avances permiten entrenar modelos con cientos de capas
  
---

## Democratización

* Antes: programar en GPUs $\Rightarrow$ lenguajes específicos (CUDA) y C++

* Desde 2010: multiples librerias (Torch, Theano, Caffe, Tensorflow) que

  1. realizan diferenciación automática
  
  2. implementan tensores y operaciones con los mismos
  
  3. hacen uso de la GPU de forma transparente

* Ahora: varias librerías de alto nivel que implementan capas de aprendizaje profundo:

  1. **Keras**
  
  2. PyTorch
  
---

## Ejemplo Keras

* Problema de regresión con 2 salidas

* Arquitectura:

.center[
![](./img/Colored_neural_network.svg)
]

---

```{r eval=FALSE}
library(keras)

model <- keras_model_sequential()

# arquitectura
model %>% 
  layer_dense(units = 4, 
              activation = 'sigmoid', 
              input_shape = c(3)) %>% 
  layer_dense(units = 2, 
              activation = 'linear')

# definir entrenamiento
model %>% compile(loss = "mse", 
                  optimizer = optimizer_sgd())

# entrenamiento
model %>% fit(X_train, y_train, 
              epochs = 10, batch_size = 128, 
              validation_size = 0.2)

# error de test
model %>% evaluate(X_test)
```
  
---

class: middle, center, inverse
# Feed-forward Neural Networks

---

## Introducción

* Hasta ahora hemos visto modelos de regresión y clasificación que recibían como input combinaciones lineales de *funciones base*.

* Para que estos modelos resulten prácticos, debemos adaptar las funciones base a los datos.

* Idea: 

  1. Fijar el número de funciones base de antemano
  
  2. Darles forma paramétrica
  
  3. Aprender parámetros usando los datos.

* **Aprender la representación**.

* *Perceptrón multicapa* o *feed-forward neural network*.

---

## Perceptrón multicapa (1)

* ¿Cómo parametrizamos las funciones de base?

* Hasta ahora

\begin{equation}
y(x,w) = f \left( \sum_{j=1}^M w_j \phi_j(x) \right)
\end{equation}

* Siendo $f$ una **activación no lineal**.

* Objetivo: parametrizar $\phi_j(x)$ y aprender los parámetros.

* Idea de las NN: parametrizar $\phi_j(x)$ de la misma manera que $y(x,w)$.

---

## Perceptrón multicapa (2)

* MLP básico: 

  1. Construír $M$ combinaciones lineales del input $x_1, \dots, x_D$:
\begin{equation}
a_j = \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}
\end{equation}
  
  2. Transformar cada activación $a_j$ usando una función de activación **no lineal** y **diferenciable**: $z_j = h(a_j)$.
  
  3. Repetir 1 y 2, tantas veces como **capas ocultas** queramos en la red.
  
  4. Por último, en la capa de salida, las activaciones se transforman con una función de activación adecuada para producir los outputs $y_k$.

* Notación: $w_{ji}$ son *pesos*, $w_{j0}$ son *biases*, $a_j$ son *activaciones*.

---

## Perceptrón multicapa (3)

* **Ejercicio**: ¿Por qué las activaciones tienen que ser funciones no lineales diferenciables?

---

## Perceptrón multicapa (4)
  
* La función de activación de la capa de salida, dependerá de la naturaleza de los datos.

* Para problemas de regresión, la activación será la identidad $y_k = a_k$.

* Para clasificación binaria (output es una probabilidad) la activación será la sigmoide $y_k = \sigma(a_k)$.

* Para clasificación multiclase, usaremos la softmax.

\begin{equation}
\text{softmax}(a)_i = \frac{e^{a_i}}{\sum_k e^{a_k}}
\end{equation}


---

## Perceptrón multicapa (6)

* ¿Funciones de activación de capas intermedias?

* Históricamente, la sigmoide.

* Hoy en día, las más conocidas son la *REctifier Linear Unit (RELU)*, tangente hiperbólica y variantes.

<center>
![:scale 100%](./img/activation.png)
</center>


---

## Perceptrón multicapa (7)

* Componiendo lo visto, obtenemos una NN de dos capas (e.g. con salida binarias)

\begin{equation}
y(w,x) = \sigma \left( \sum_{j=0}^M w_{j}^{(2)} h \left( \sum_{i=0}^D w_{ji}^{(1)} x_i\right) \right)
\end{equation}

* El proceso de evaluar esta función se denomina *forward propagation*.
.center[
![:scale 60%](./img/neuron.svg)

[Fuente](https://victorzhou.com/blog/intro-to-neural-networks/)
]

---

## Perceptrón multicapa (8)

* Gráficamente...

<center>
![:scale 100%](./img/nn.jpg)
</center>


