<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Modelos lineales: regresión</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alberto Torres Barrán" />
    <meta name="date" content="2019-04-08" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Modelos lineales: regresión
## Curso de aprendizaje automático para el INE
### Alberto Torres Barrán
### 2019-04-08

---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      Xcal: "{\\mathcal{X}}",
      Xbf: "{\\mathbf{X}}",
      Hbf: "{\\mathbf{H}}",
      Rbb: "{\\mathbb{R}}"
    },
    extensions: ["AMSmath.js","AMSsymbols.js"]
  }
});
</script>

## Modelos lineales

Variables provienen de múltiples fuentes:
* variables quantitativas
* transformaciones (logaritmo, raiz cuadrada, etc.)
* expansiones de base, `\(x_2 = x_2^2\)`
* variables *dummy*
* interacciones entre variables, `\(x_3 = x_1 \times x_2\)`

Pero siempre: modelo lineal en los parámetros

---

## Regresión lineal y logística

La salida `\(y\)` es continua, `\(y \in \Rbb\)`

* Regresión lineal (*MSE* o *RSS*) `$$\min_w\, ||y - \Xbf w||_2^2$$`

--

La salida `\(y\)` es discreta, `\(y \in \{ 0, 1 \}\)`

* Regresión logística (*log-loss*) `$$\min_w\, -(y^T \log[\sigma(\Xbf w)] + (1 - y)^T \log[1 - \sigma(\Xbf w)])$$`

???

La función de pérdida log-loss es un poco distinta a como la vimos antes
  * Tiene un menos delante, porque queremos minimizar y no maximizar la verosimilitud
  * Propiedad de la función sigmoidea `\(\sigma(-x) = 1 - \sigma(x)\)`

---

## Generalized linear models (GLM)

* Generalización de la regresión lineal que permite distribuciones de errores distintas de la distribución normal.

* Componentes:
  
  * Distribución de `\(Y_i\)` con media `\(\mu_i\)`
  * Predictor lineal, `$$g(\mu_i) = w^T x_i$$` donde `\(g(\cdot)\)` es la función de media

* La función de media proporciona la relación entre la media de la distribución y el predictor lineal

* El inverso de la función de media, `\(g^{-1}(\cdot)\)` se conoce con el nombre de **función de enlace**

---

##Ejemplo: distribución binomial

* La regresión logística es un caso particular de GLM donde la distribución de `\(Y\)` es la binomial

* La función de media es la logística,

`$$\mu = g^{-1}(w^T x_i) = \frac{1}{1 + \exp(-w^T x_i)}$$`

* La función de enlace es la inversa de la anterior,

`$$w^T x_i = g(\mu) = \ln\left(\frac{\mu}{1 - \mu}\right)$$`

* Para cada distribución, hay una función de enlace "canónica" que es la que se usa habitualmente

---

## Ejemplo: distribución de Poisson

* Esta distribución está indicada cuando queremos modelizar una variable de salida entera y no real (por ej. conteos)

* Función de media

`$$\mu = \exp(w^T x_i)$$`

* Función de enlace

`$$w^T x_i = \ln(\mu)$$`

* Otras distribuciones posibles son la Gamma, Exponencial, Multinomial, etc.

---

## GLMs en R

* La función para ajustar modelos lineales generalizados es `glm()`

* Tiene los mismos argumentos principales que `lm()`, pero además tenemos que especificar la distribución de la variables dependiente con el parámetro `family`

* Por defecto se usa la función de enlace "canónica", pero esto se puede modificar (ver ayuda)

* Implementa el algoritmo IRLS (Newton-Raphson), que se puede generalizar para cualquier GLM donde la distribución pertenece a la familia exponencial

Ejemplo: regresión logística


```r
library(MASS)
fit &lt;- glm(type ~ ., data=Pima.tr, family=binomial)
```

---

## Problemas de mínimos cuadrados

1. Calidad de predicción: 
  
  * poco sesgo pero potencialmente mucha varianza
  
  * podemos mejorar las predicciones reduciendo el valor de algunos coeficientes
  
  * aumenta ligeramente el sesgo pero disminuye mucho la varianza

2. Interpretación: 

  * el valor de los coeficientes nos da una idea de las variables mas relevantes
  
  * nos gustaria encontrar un subconjunto de los mejores

???

Teorema de Gauss-Markov

El estimador de minimos cuadrados es BLUE

---

## Regularización

* Regresión *ridge* (*MSE* + regularización `\(l_2\)`): `$$\min_w\, ||y - \Xbf w||_2^2 + ||w||_2^2$$`

--
  
* ¿Regresión logística ridge?

--

* ¿Otras funciones de regularización?

---

class: middle, center, inverse

# Métodos de seleccion

---

## Regresión *best subset*

* Mantenemos solo un subconjunto de las variables y eliminamos el resto del modelo

* Para `\(k \in \{0, 1, 2, \dots, d\}\)` se resuelve `$$\min_w\, || y - \Xbf w ||_2^2 \quad \mbox{s.t.}\; ||w||_0 \leq k$$`
donde `\(||w||_0 = \sum_{i=1}^d{\mathbb{I}(w_i \neq 0)}\)`

* `\(\mathbb{I}(\cdot)\)` es la función indicatriz (cuenta el número de elementos distintos de 0)

* La restricción hace que el problema sea NP-completo, `$$C_{d,k}=\binom{d}{k} = \frac{d!}{k!(d-k)!}$$`

* Algoritmos clásicos pueden resolver `\(d \approx 30\)`

* Avances recientes, [(Bertsimas et al., 2015)](https://arxiv.org/pdf/1507.03133.pdf): `\(d \in [100, 1000]\)` 

???

`\(C_{30, 10} \approx 30M\)`

---

class: center, middle

![](./img/best_subset.png)

???

`\(C_{8, 2} = 28\)`

`\(C_{8, 3} = 56\)`

`\(C_{8, 4} = 70\)`

---

## Best subset en R

Paquete `leaps`

---

## Regresión *stepwise*

* *Forward-Stepwise*:
  
  1. Añadir al modelo la variable que proporciona mejor ajuste
  2. Repetir hasta añadir `\(k\)` variables

* *Backward-Stepwise* 

  1. Empezar con las `\(d\)` variables
  2. Eliminar iterativamente la menos relevante para el ajuste 

* Algoritmos avariciosos

* No buscan entre todas las posibles combinaciones de subconjuntos de tamaño `\(k\)`

* En cada paso solo se ajustan `\(d-k\)` modelos

???

Mas sesgo pero potencialmente menos varianza, al imponer mas estructura
(no se busca el mejor)

---

## *Stepwise* en R

* Función `step`

* Procedimiento hibrido: se procede como *forward* pero en cada paso está la opción de eliminar alguna variable añadida previamente

* Añade o elimina variables en grupos (por ej. si son variables *dummy*)

* Selecciona automáticamente el valor óptimo de `\(k\)`

---

## Selección de k

* Secuencia de modelos indexada por `\(k\)` (igual que *best subset*)

* Elegir `\(k\)` como el que minimiza el error de validación cruzada 

* Validación cruzada: estimación del error de generalización o error *extra-sample*

* Error de entrenamiento es demasiado optimista (error *in-sample*)

* Alternativa: cuantificar el "optimismo" y minimizarlo (AIC, BIC y derivados)

???

ESL secciones 7.4 en adelante

---

class: inverse, middle, center

# Métodos de reducción

---

## Regresión *rigde*

ESL sección 3.4.1

---

## Lasso: motivación

* Métodos de selección: 
   
   * ![:colorText green](modelos interpretables)
    
   * ![:colorText red](proceso discreto, las variables están incluidas o no)


* Regresión *ridge*: 

  * ![:colorText green](proceso continuo, todos los coeficientes se reducen)

  * ![:colorText red](rara vez son exactanente 0, modelos no interpretables)


* Lasso es una técnica intermedia:

  * ![:colorText green](reduce algunos coeficientes)
  
  * ![:colorText green](pone el resto a 0)

???

Modelos seleccion: variables entran o salen del modelo

---

## Lasso: formulación

* Problema optimización: `$$\min_w\, || y - \Xbf w ||_2^2\quad \text{s.t. }\; ||w||_1 \leq t$$`

* Equivalente: `$$\min_w\, || y - \Xbf w ||_2^2 + \lambda ||w||_1$$`

* `\(\lambda\)` o `\(t\)` son hiper-parámetros

  * `\(\uparrow \lambda\)` o `\(\downarrow t\)`, se reducen los coeficientes (más regularización)

  * `\(\downarrow \lambda\)` o `\(\uparrow t\)`, aumentan los coeficientes (menos regularización)
  
* `\(t\)` suficientemente pequeño (o `\(\lambda\)` suficientemente grande), algunos coeficientes = 0

---

class: middle, center

![:video 600 600](./img/lasso.mp4)


---

class: middle, center

![](./img/orthonormal.png)

???

`\(X^T X = I\)`, no ocurre en la practica pero interesante para ver la diferencia entre los estimadores

---

class: middle, center

![](./img/constraint_regions.png)

---

* Comparación Lasso, *best subset* y *forward stepwise* [(Hastie et al., 2017)](https://arxiv.org/pdf/1707.08692.pdf)

* Experimentos en [Github, best-subset](https://github.com/ryantibs/best-subset/)

* [*Relaxed* Lasso](https://stat.ethz.ch/~nicolai/relaxo.pdf)

???

[Relaxed Lasso](https://stats.stackexchange.com/questions/122955/why-is-relaxed-lasso-different-from-standard-lasso)

---

## Lasso: limitaciones

1. Si `\(d &gt; n\)`, como mucho `\(n\)` coeficientes son `\(\neq 0\)`

    * Limitación desde el punto de vista de selección de variables

2. Variables con correlación alta `\(\Longrightarrow\)` Lasso selecciona una "aleatoriamente"

3. Si `\(n &gt; d\)` y hay variables con correlación alta `\(\Longrightarrow\)` error de Ridge &lt; error de Lasso

---

## Otras penalizaciones

* Podemos generalizar Lasso, Ridge y Best subset como `$$\min_w || y - \Xbf w ||_2^2 + \lambda || w ||_p^p$$`
donde, `$$|| w ||_p^p = \sum_{i=1}^d{|x_i|^p}$$`

* `\(0 \leq p &lt; 1\)`, no convexas

* `\(p = 1\)`, convexa y no diferenciable

* `\(p &gt; 1\)` convexas y diferenciables

---

## Interpretación Bayesiana

* Regularización = distribución a priori de los parámetros `\(w\)`

* Ridge regressión: distribución Normal

* Lasso: distribución de Laplace, `\(\tau = 1/\lambda\)` `$$f(w) = \frac{1}{2\tau} \exp\bigg(-\frac{|w|}{\tau}\bigg)$$`

* Estimadores: máximo de la distribución a posteriori (MAP)
  
  * Ridge: coincide con la media
  * Lasso y Best subset: moda

---

class: center, middle

![](./img/laplace.svg)

[Wikipedia, "Laplace distribution"](https://en.wikipedia.org/wiki/Laplace_distribution)

---

class: middle, center

![](./img/penalties.png)

---

## Elastic Net

* Combina regularización Lasso y Ridge: `$$\min_w\, || y - \Xbf w ||_2^2 + \lambda_1 || w ||_1 + \lambda_2 || w ||_2^2$$`

* Otra parametrización con `\(\alpha \in (0, 1)\)`: `$$\min_w\, || y - \Xbf w ||_2^2 + \lambda(\alpha || w ||_1 + (1 - \alpha) || w ||_2^2)$$`

* Equivalentes con 

  * `\(\alpha = \lambda_1/(\lambda_1 + \lambda_2)\)` 
  
  * `\(\lambda = \lambda_1 + \lambda_2\)`
  
* Selecciona variables y reduce el resto de coeficientes

---

class: center, middle

![:scale 85%](./img/penalties.svg)

???

Ventaja de parametrizacion 2: `\(\lambda\)` te da la distancia al origen, y `\(\alpha\)` la curvatura

---

## Notación

* Lasso (Elastic Net) suelen hacer referencia a: minimizar MSE + norma `\(l_1\)` (+ norma `\(l_2\)`)

* MSE puede reemplazarse por otras funciones de pérdida

* Por ej. cualquier GLM

* En esos casos hablamos de regresión logística, Poisson, Gamma + regularización Ridge/Lasso/Elastic Net

---

## Elastic Net en R

Paquete `glmnet`

---

class: middle, center, inverse

# Métodos de reducción

---

## Principal Component Regression

---

## Partial Least Squares

---

## Algoritmos

 * FISTA

 * Descenso coordinado

---
 
## Variantes del Lasso

Group Lasso, Fused Lasso, Generalized Lasso, Relaxed Lasso
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
