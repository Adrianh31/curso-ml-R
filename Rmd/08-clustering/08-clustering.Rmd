---
title: "Análisis de conglomerados (Clustering)"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Víctor Gallego y Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    
---

class: middle, center, inverse

# Introducción

---

## Introducción

* Objetivo: agrupar un conjunto de ejemplos en *clusters*.

* Ejemplos del mismo cluster más parecidos entre sí que ejemplos de diferentes clusters.

--

* Otro objetivo: ordenar clusters en jerarquías naturales.

* En cada nivel de la jerarquía, objetos en el mismo grupo más similares que objetos en grupos distintos.

--

* Estadístico descriptivo: ¿consisten los datos en un conjunto de subgrupos cada uno con propiedades diferentes?

--

* Fundamental: escoger medida de distancia (disimilaridad).
* Esto depende del problema concreto (similar a elección de coste en aprendizaje supervisado).

---
## Matrices de proximidad

---
## Medidas de disimilaridad entre atributos

* Datos: $x_{ij}$ con $i = 1,2,\dots,N$ y $j=1,2,\dots, p$. $N$ ejemplos con $p$ atributos.

* Técnicas populares de clustering toman matriz de disimilaridad como input. ¿Cómo construírla?

--

* Construír medidas de disimilaridad por pares.
* $d_j(x_{ij}, x_{i'j})$: disimilaridad entre valores de atributo $j$.

\begin{equation}
D(x_i, x_{i'}) = \sum_{j=1}^p d_j(x_{ij}, x_{i'j}) 
\end{equation}

* No hay por qué pesar todos los atributos igual...

---
## Medidas de disimilaridad entre atributos

* En función del tipo de atributo:

  1. Variables cuantitativas: 
  
\begin{equation}
d(x_i, x_{i'}) = l(|x_i - x_{i'}|)
\end{equation}

con $l$ función monótona creciente. Error cuadrático o absoluto. También es posible usar correlaciones (medida de similaridad):

\begin{equation}
\rho(x_i, x_{i'}) = \frac{\sum_{j} (x_{ij} - \bar{x}_i) (x_{i'j} - \bar{x}_{i'})}{\sqrt{\sum_{j} (x_{ij} - \bar{x}_i)^2 \sum_{j} (x_{i'j} - \bar{x}_{i'})^2}}
\end{equation}
---
## Medidas de disimilaridad entre atributos

* En función del tipo de atributo:

  1. Variables categóricas: Si tienen $M$ categorías, construír matriz $M \times M$.
  
\begin{eqnarray*}
L_{jj} = 0 & ~ & L_{jj'} = 1
\end{eqnarray*}
  
  Valores distintos de 1 pueden utilizarse para enfatizar unos errores más que otros.



---
## Medidas de disimilaridad entre objetos

* Procedimiento para combinar $p$ medidas de disimilaridad entre atributos en una medida de disimilarida entre ejemplos $D(x_i, x_{i'})$.

* Combinación convexa

\begin{eqnarray*}
  D(x_i, x_{i'}) = \sum_{j=1}^p \omega_j \cdot d_j(x_{ij}, x_{i'j}); &~& \sum_{j=1}^p \omega_j = 1
\end{eqnarray*}

* Elección de pesos, depende del problema en cuestión.

---
## Medidas de disimilaridad entre objetos

* Ojo: dar el mismo peso a todos los atributos, no signfica que todos contribuyan en la misma cantidad a la medida total de disimilaridad.

* Veámoslo:

\begin{equation}
\bar{D} = \frac{1}{N^2} \sum_{i=1}^N \sum_{i'=1}^N D(x_i, x_{i'}) = \sum_{j=1}^p \omega_j \bar{d}_j d_j(x_{ij}, x_{i'j})
\end{equation}

Con 

\begin{equation}
\bar{D} = \bar{d}_j = \frac{1}{N^2} \sum_{i=1}^N 
\end{equation}

* La influencia relativa de la variable $j$ es $\omega_j \bar{d}_j$ y si $\omega_j \sim 1/\bar{d}_j$, entonces todas tienen la misma influencia.

* **Ejercicio**: Si se usa distancia Euclídea $\bar{d}_j ~ 2\cdot \text{var}_j$.

---
## Medidas de disimilaridad entre objetos

* No suele ser conveniente dar a todas las variables la misma influencia.

* Importante: estudiar cada problema en concreto para construír tanto las medidas de disimilaridad entre atributos como los pesos.

* Aunque no se diga: esta es la parte más importante (y más costosa).


---
class: middle, center, inverse
# Algoritmos de clustering

---
## Tipos de algoritmos de clustering

* Algoritmos combinatorios: trabajan con los datos observados sin hacer referencia al modelo probabilístico subyacente.

* Modelos de mixturas: asumen que datos son muestas iid de una mixtura de densidades de probabilidad. Cada componente describe un cluster.

* *Mode Seekers*: modelos no paramétricos que estiman modas.

---
class: middle, center, inverse

# Algoritmos de clustering combinatorios: K-Means

---
## Algoritmos combinatorios

* Asignan cada observación a un cluster, sin preocuparse de la distribución de probailidad de subyacente.

* Etiquetamos cada observación con $i \in \lbrace 1,2,\dots,N   \rbrace$

* Postulamos un número fijo de clusters $K<N$, etiquetados con $k \in \lbrace 1, \dots, K$.

--

* Objetivos construír función de asignación $k = C(i)$ para cada $i$.

* Buscar asignación que minimice **dispersión dentro del cluster**

\begin{equation}
W(C) = \frac{1}{2} \sum_{k=1}^K \sum_{i:C(i)=k} \sum_{i':C(i')=k} d(x_i, x_{i'})
\end{equation}

---
## Algoritmos combinatorios

* La dispersión total es

\begin{eqnarray}
T &=& \frac{1}{2} \sum_{i=1}^N \sum_{i'=1}^N d(x_i, x_{i'}) = \frac{1}{2} \sum_{k=1}^K \sum_{i:C(i)=k} \left[ \sum_{i':C(i')=k} d(x_i, x_{i'}) + \sum_{i':C(i')\neq k} d(x_i, x_{i'}) \right] \\
&=& W(C) + B(C)
\end{eqnarray}

* $B(C)$ es la **dispersión entre clusters**.

* Minimizar $W(C)$ es equivalente a maximizar $B(C)$.

--

* Optimiziación por enumeración complete **inviable**.
* $N=19$, $K=4$ número de asignaciones del orden de $10^{10}$.

---
## Algoritmos combinatorios

* Estrategia: *iterative greedy descent*
  1. Asignación inicial.
  2. Iterativamente, cambiar asignación tal que se diminuya $W(C)$.
  3. Parar cuando se deje de mejorar.
  
* Distintos algoritmos de clustering en función del paso 2.

* Asegurada convergencia a óptimo local...

---
## K-Means

* Todas las variables son cuantitativas. 

* Disimilaridad = distancia Euclídea

\begin{equation}
d(x_i, x_{i'}) = \Vert x_i - x_{i'} \Vert^2
\end{equation}

* La distancia Euclídea con pesos puede usarse redefiniendo $x_{ij}$.
--

* En este caso,

\begin{equation}
W(C) = \sum_{k=1}^K N_k \sum_{i:C(i) = k} \Vert x_i - \mu_k \Vert^2
\end{equation}

* Como $\mu_k = \arg\min_m \sum_{i:C(i) = k} \Vert x_i - m \Vert^2$, vemos que la asignación óptima puede obtenerse resolviendo

\begin{equation}
\min_{C, \lbrace m_k \rbrace_1^K}  \sum_{k=1}^K N_k \sum_{i:C(i) = k} \Vert x_i - m_k \Vert^2
\end{equation}

---
## K-means

* K-means resuelve utilizando *descenso coordenado*:

  1. Fijar asignación $C$ y minimizar respecto $\lbrace m_1, \dots m_K \rbrace$, asignando las medias de cada cluster.
  2. Fijar las medias $\lbrace m_1, \dots m_K \rbrace$ y minimizar asignando cada observación al cluster de la media más cercana.
  
  \begin{equation}
C(i) = \arg\min_{k}  \Vert x_i - m_k \Vert^2
\end{equation}

  3. Iterar 1 y 2 hasta que no cambien las asignaciones.

---
## Cuantización vectorial

* Técnica de teoría de la señal para aproximar datos de alta dimensión.

* Utilizada para compresión de datos.

* Idea: dividir conjunto grande de puntos en grupos y aproximar cada grupo por su centroide.

* En compresión de imágenes:

  1. Dividir imagen de NxN pixels en grupos de kxk pixels (k < N).
  2. Considerar cada grupo como un vector de dimensión kxk.
  3. Aplicar K-means y aproximar cada grupo por su centroide más próximo. (Conjunto de centroides se conoce como codebook).

* Puede usarse también para imputar datos.
  

---
class: middle, center, inverse

# Algoritmos de clustering: K-Mediodes


---

class: middle, center, inverse

# Algoritmos de clustering: Clustering jerárquico


---

class: middle, center, inverse

# Clustering Probabilístico