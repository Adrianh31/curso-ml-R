---
title: "Modelos Gráficos Probabilísticos (PGMs)"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Víctor Gallego y Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    
---

class: middle, center, inverse

# Introducción

---

## Representar gráficamente distribuciones de probabilidad. Ventajas

* Forma simple de visualizar la estructura de modelos probabilísticos.

* Diseño y motivación de nuevos modelos.

* Facilita comprender propiedades de los modelos, como la independencia condicional.

* Cálculos complejos pueden ser expresados en términos de manipulaciones gráficas sencillas.



---

class: middle, center, inverse

# Redes Bayesianas

---
## Introducción

* Podemos representar cualquier distribución de probabilidad utilizando un **grafo acíclico dirigido** (DAG).

* Cada nodo es una variable aleatoria de la distribución.

* Arcos entre nodos representan dependencias condicionales.

* Dado un DAG, la distribución de probabilidad conjunta es

\begin{equation}
p(x) = \prod_{k=1}^K p(x_k \vert \text{pa}_k)
\end{equation}

---

## Introducción

* **Ejercicio**: Factoriza y representa gráficamente la distribución $p(a,b,c)$.

* **Ejercicio**: ¿A qué distribución corresponde este DAG?

<center>
![:scale 50%](./img/pgm1.png)
</center>


---

## Notación

* Para expresar multiplicidad de variables aleatorias se utiliza la siguiente notación


<center>
![:scale 50%](./img/plate1.png)
![:scale 50%](./img/plate2.png)
</center>

---

## Notación

* Los parámetros deterministas se representan usando círculos sólidos.

* Las variables observadas se colorean.

<center>
![:scale 50%](./img/full1.png)
</center>

* **Ejercicio**: ¿Qué modelo representa esta Red Bayesiana?

---

## Independencia Condicional

* Decimos que $a$ y $b$ son condicionalmente independientes dado $c$ si $p(a \vert b,c) = p(a \vert c)$ o $p(a,b \vert c) = p(a \vert c) p(b \vert c)$.

* Los PGMs permiten identificar las propiedades de independencia condicional de la distribución conjunto de forma automática.

---

## Ejemplo 1

* Nodo *tail-to-tail*
* Si se condiciona en $c$ este nodo bloquea el camino entre $a$ y $b$, y se cumple independencia condicional.
<center>
![:scale 40%](./img/ci1.png)
</center>

<center>
![:scale 40%](./img/ci2.png)
</center>

---

## Ejemplo 2

* Nodo *head-to-tail*
* Si se condiciona en $c$ este nodo bloquea el camino entre $a$ y $b$, y se cumple independencia condicional.
<center>
![:scale 40%](./img/ci3.png)
</center>

<center>
![:scale 40%](./img/ci4.png)
</center>


---

## Ejemplo 3

* Nodo *head-to-head*
* Si se $c$ **no se observa** bloquea el camino entre $a$ y $b$ y estas son independientes.
* Si se $c$ **se observa** el camino se desbloquea: $a$ y $b$ y son dependientes.
<center>
![:scale 35%](./img/ci5.png)
</center>

<center>
![:scale 35%](./img/ci6.png)
</center>

---
## D-separación

* Consideremos un DAG general y sean $A$, $B$ y $C$ conjuntos de nodos diferentes.

* Queremos determinar si $A$ y $B$ son condicionalmente independientes dado $C$.

* Consideremos todas las trayectorias entre $A$ y $B$. Diremos que alguna de estas está bloqueada si:
  * Las flechas del camino encuentran un nodo *head-to-tail* o *tail-to-tail* y este está en $C$.
  
  * Las flechas del camino encuentran un nodo *head-to-head* y ni el nodo ni sus descendientes están en $C$.

* Si todos los caminos entre $A$ y $B$ están bloqueados, entonces $A$ y $B$ están $d$-separados por $C$ y son condicionalmente independientes dado $C$.

---
## D-separación
* Determinar si $a$ y $b$ son condicionalmente independientes dados los nodos observados en cada caso.

<center>
![:scale 100%](./img/ejci.png)
</center>

---
## D-separación
* Dibuja la red bayesiana de un modelo de Naive-Bayes

* Estudia la independencia condicional entre $t_n$ y $\hat{t}$.

<center>
![:scale 50%](./img/ci_ex.png)
</center>

---
## D-separación

* A efectos de $D$-separación, los parámetros se comportan como variables aleatorias observadas.

* Como nunca tienen padres son siempre nodos *tail-to-tail* y bloquean todas las trayectorias en las que intervienen.

* El conjunto de distribuciones que pueden ser expresadas en términos de la factorización implicada por un DAG, se denomina $\mathcal{DF}$.

* $\mathcal{DF}$ coincide con todas las distribuciones que cumplen las propiedades de independencia condicional del DAG.

* Se denomina *Markov Blanket* de una variable $x_i$ al conjunto de padres, hijos y co-padres de $x_i$.

* Demostrar que para una distribución arbitraria $p(x_1, \dots, x_D)$, $p(x_i \vert x_{\lbrace j \neq i \rbrace})$ depende únicamente de las variables del *Markov Blanket* de $x_i$.

---
class: middle, center, inverse

# Markov Random Fields

---

class: middle, center, inverse

# Inferencia Exacta en Modelos Gráficos

---
class: middle, center, inverse

# Algoritmo suma-producto

--- 
## Introducción

* Usando el marco de grafos de factores, derivaremos un algoritmo eficiente de **inferencia exacta** aplicable a grafos con estructura de árbol.

* Nos centraremos en evaluar marginales locales sobre nodos o conjuntos de nodos.

* Por sencillez, supondremos que todas las variables son discretas (la extensión a caso contínuo es trivial).

* Asumiremos que el grafo original es un **árbol no dirigido**, un **árbol dirigido** o un **poli-árbol**.

* En estos casos, el grafo de factores tiene estructura de árbol.

---
## Objetivos

* Primero de todo convertimos el grafo original en un grafo de factores.

* El objetivo es explotar la estructura del grafo para:

  1. Obtener un algoritmo eficiente de inferencia exacta para encontrar las marginales.
  
  2. En el caso de computar muchas marrginales, *compartir* los cálculos de manera eficiente.
  
---
## Algoritmo suma-producto (1)

* Empezamos por encontrar $p(x)$ para un nodo en concreto.

\begin{equation}
p(x) = \sum_{\textbf{x} / x} p(\textbf{x} ) = \sum_{\textbf{x} / x} \prod_{s} f_s(x_s)
\end{equation}

* La estructura de árbol permite dividir los factores en la conjunto en grupos, cada uno asociado con un factor vecino de $x$.

---
## Algoritmo suma-producto (2)

* Llamamos a $\text{ne}(x)$ al conjunto de nodos factor vecinos de $x$.

* $X_s$: todas las variables **en el subárbol** conectado a $x$ via un nodo factor $f_s$.

* $F_s(x, X_s)$ el producto de **todos los factores** del grupo asociado a $f_s$.

* Entonces:

\begin{equation}
p(\textbf{x} ) = \prod_{s \in \text{ne}(x) } F_s(x, X_s)
\end{equation}

<center>
![:scale 40%](./img/sp1.png)
</center>

---
## Algoritmo suma-producto (3)

* Sustituyendo, cambiando orden suma producto y definiendo **mensajes** entre $f_s$ y x

\begin{equation}
p(x) = \prod_{s \in \text{ne}(x) } \left[ \sum_{X_s} F_s(x, X_s) \right] := \prod_{s \in \text{ne}(x) } \mu_{f_s \rightarrow x} (x)
\end{equation}

* Necesitamos evaluar los mensajes. Vemos que cada factor $F_s(x, X_s)$ es a su vez un grafo de factores y puede ser factorizado.

---
## Algoritmo suma-producto (4)

* Denotamos las variables asociadas a $f_s$ (a parte de $x$) como $x_1, \dots, x_M$.

\begin{equation}
F_s(x, X_s) = f_s(x, x_1, \dots, x_M) G_1(x_1, X_{s1}) \dots G_M(x_M, X_{sM})
\end{equation}

<center>
![:scale 40%](./img/sp2.png)
</center>

---
## Algoritmo suma-producto (5)

* Substituyendo y llamando $\text{ne}(f_s)$ al conjunto de nodos variable vecinos de $f_s$.

\begin{eqnarray}
\mu_{f_s \rightarrow x} (x) &=& \sum_{X_s} F_s(x, X_s) \\
&=& \sum_{x1,\dots,x_M} f_s(x, x_1, \dots, x_M) \prod_{m \in \text{ne}(f_s) / x} \left[ \sum_{X_{sm}} G_{m} (x_m, X_{sm})\right] \\
&=& \sum_{x1,\dots,x_M} f_s(x, x_1, \dots, x_M) \prod_{m \in \text{ne}(f_s) / x} \left[\mu_{x_m \rightarrow f_s} (x_m) \right]
\end{eqnarray}
 
* Existen dos tipos de mensajes: variable-factor y factor-variable.

* Evaluar mensaje de nodo factor a nodo variable requiere evaluar el **producto de mensajes recibidos** por el nodo factor, **multiplicar factor asociado** a este nodo y **marginalizar en variables asociadas a mensajes recibidos**.

---
## Algoritmo suma-producto (6)

* Para cerrar círculo, evaluamos los mensajes variable-factor.

<center>
![:scale 40%](./img/sp3.png)
</center>

* Vemos que $G_m(x_m, X_{sm})$ admite la factorización

\begin{equation}
\prod_{l \in \text{ne}(x_m)/f_s } F_l(x_m, X_{ml}) 
\end{equation}

---
## Algoritmo suma-producto (6)

* Con esto

\begin{eqnarray}
\mu_{x_m \rightarrow f_s} (x_m) &=& \prod_{l \in \text{ne}(x_m)/f_s } \left[ \sum_{X_{ml}} F_l (x_m, X_{ml})\right] \\
&=& \prod_{l \in \text{ne}(x_m)/f_s } \mu_{f_l \rightarrow x_m} (x_m)
\end{eqnarray}

* Para evaluar mensajes mandados por variable nodo a variable factor, calcular producto de todos los mensajes que llegan a la variable nodo.

---
## Algoritmo suma-producto (7)

* Cada mensaje puede ser calculado recursivamente en términos de otros mensajes.

* ¿Cómo empezar la recursión?

* Vemos $x$ como el nodo raíz, y nos vamos a loas nodos hoja.


<center>
![:scale 70%](./img/sp4.png)
</center>


---
## Algoritmo suma-producto (8)

* Para calcular todas las marginales de forma eficiente:
  
  1. Escoger nodo arbitrario como raíz.
  
  2. Calcular y propagar mensajes de las hojas hasta la raíz, guardando todos los mensajes recibidos en cada nodo.
  
  3. Calcular y propagar mensajes de la raíz hasta las hojas, guardando todos los mensajes recibidos en cada nodo.
  
  4. Calcular el producto de mensajes recibidos en cada nodo y normalizar si es necesario.
  
---
## Algoritmo suma-producto (9)

* **Ejercicio** :  Demostrar que las marginales sobre variables asociadas a un factor se pueden escribir como

\begin{equation}
p(\boldsymbol{x_s}) = f_s(\boldsymbol{x_s}) \prod_{m \in \text{ne}(f_s)} \mu_{x_m \rightarrow f_s} (x_m)
\end{equation}

---
## Algoritmo suma-producto (10)

* Cuando hay variables observadas, particionamos $\textbf{x}$ en variables observadas $\textbf{v}$ y ocultas $\textbf{h}$.

* Sean $\boldsymbol{\hat{v}}$ los valores observados. Redefinimos la conjunta como

\begin{equation}
p(\boldsymbol{x}) \prod_i I(v_i, \hat{v}_i)
\end{equation}

* Que no es más que la versión sin normalizar de $p(\boldsymbol{h} \vert \boldsymbol{v} = \boldsymbol{\hat{v}})$.

* Con el agoritmo suma-producto podemos calcular las versiones sin normalizar de

\begin{equation}
p(h_i \vert \boldsymbol{v} = \boldsymbol{\hat{v}})
\end{equation}

* Que es barato de normalizar.
---

class: middle, center, inverse

# Algoritmo max-suma


---
## Inferencia en grafos generales

* El algoritmo *junction tree* generaliza el marco visto a grafos arbitrarios (con ciclos), pero no es eficiente en general.

* Otra alternativa es usar métodos aproximados.

* *Loopy belief propagation* propone usar el algoritmo suma producto aunque no se haya ciclos.

* La información fluye indefinidamente por los ciclos. En algunos casos se converge (no garantizado!).








