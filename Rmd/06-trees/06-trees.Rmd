---
title: "Algoritmos de Árboles y Colectividades"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Víctor Gallego y Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    
---
class: middle, center, inverse

# Intro a métodos basados en árboles


---
class: middle, center, inverse

# Boosting

---

## Aprendiendo de los errores

* Dado un dataset de entrenamiento $\lbrace (x_i, y_i) \rbrace_{i=1}^N$, ajustamos un modelo $G(x)$ y calculamos la tasa de error sobre el conjunto de entrenamiento
\begin{equation*}
err = \frac{1}{N} \sum_{i=1}^N I(y_i \neq G(x_i))
\end{equation*}

* Partiendo de que $G(x)$ es un clasificador débil, ¿cómo podemos hacer que además vaya corrigiendo sus errores?

--

* **Boosting**: iremos aplicando el algoritmo a versiones modificadas de los datos de entrenamiento, obteniendo una secuencia de clasificadores

\begin{equation*}
G_m(x),\quad m=1,\ldots,M
\end{equation*}

---

## AdaBoost.M1 (1997)

* Remuestrear los datos de entrenamiento, dando mayor pesado a las instancias mal clasificadas.

* Algoritmo

* Terminar con que AdaBoost es un modelo aditivo con una funcion de coste exponencial.

---

## Gradient Boosting

* Los árboles son ideales para combinarlos con boosting.

---

## Hiperparámetros

* $J_m$, tamaño de cada árbol: se tiende a escoger $J = J_m$ para simplificar complejidad.

  * $J = 2$: no interacciones (solo hay una decisión/subdivisión).
  
  * $J = 3$: interacciones entre dos variables (subdivisiones sucesivas).
  
  * Con $J$ se permiten hasta $J-1$ interacciones.
  
  * Según ESL, típicamente escoger $4 \leq J \leq 8$.
  
--
  
* $M$, número de iteraciones/árboles.

  * El error de entrenamiento se podría reducir arbitrariamente con $M \rightarrow \infty$.
  
  * Ajustarlo mediante un conjunto de validación, y dejar de iterar cuando el error de validación no mejore tras unas pocas iteraciones (**early stopping**).
  
---

## Regularización

* $\eta \in \left[ 0, 1\right]$, tasa de muestreo/subsampling:

  * En lugar de estimar el gradiente sobre todo el conjunto de entrenamiento, $\mathcal{O}(N)$,
  
  * lo hacemos con una muestra aleatoria uniforme, $\mathcal{O}(\eta N)$ (como en SGD).
  
  * Típicamente $\eta \leq 1/2$.
  
--

* $\nu \in \left[ 0, 1\right]$, parámetro de encogimiento/shrinkage:

  * Se sustituye $f_m(x) = f_{m-1}(x) + \sum_{j=1}^{J_m} \gamma_{jm} I (x \in R_{jm})$
  
  * por $f_m(x) = f_{m-1}(x) + \nu \sum_{j=1}^{J_m} \gamma_{jm} I (x \in R_{jm})$.
  
  * Controla la tasa de aprendizaje del boosting.
  
  * Empíricamente $\nu$ bajo favorece generalización (bajo error en test), aunque hace que aumente el número de iteraciones $M$.
  

---

## Interpretabilidad

### Importancia de variables

* Para un solo árbol, Breiman propuso como medida de **importancia de una variable** $X_l$

\begin{equation*}
\mathcal{I}_l^2 (T) = \sum_{t=1}^{J-1} \hat{i}_t^2 I(v(t) = l),
\end{equation*}

(En cada nodo $t$, una de las variables $X_{v(t)}$ es la escogida para hacer el corte en dos subregiones. Tal variable escogida es la que maximiza la mejora estimada en el riesgo cuadrático $\hat{i}_t^2$)


* Para un conjunto de $M$ árboles, podemos promediarla

\begin{equation*}
\mathcal{I}_l^2  = \frac{1}{M} \sum_{m=1}^{M} \mathcal{I}_l^2 (T).
\end{equation*}

---

## Interpretabilidad

### Gráficos de dependencia parcial

* Dividimos las variables predictoras $X = (X_1, X_2, \ldots, X_p)$ en dos grupos disjuntos:

\begin{equation*}
X = (X_{\mathcal{S}}, X_{\mathcal{C}})\mbox{ tal que } \mathcal{S} \cup \mathcal{C} = \lbrace 1, 2, \ldots, p \rbrace. 
\end{equation*}

* En lugar de inspeccionar la respuesta (por ej, probabilidad de +) $f(X)$, representamos una aproximación definida sobre $X_{\mathcal{S}}$:

\begin{equation*}
f_{\mathcal{S}} (X_{\mathcal{S}}) = \mathbb{E}_{X_{\mathcal{C}}} \left[ f(X_{\mathcal{S}}, X_{\mathcal{C}})\right] \approx \frac{1}{N} \sum_{i=1}^N f(X_{\mathcal{S}}, x_{i\mathcal{C}})
\end{equation*}

* $|\mathcal{S}|$ bajo para poder visualizarlo.

* Ejemplo en los ejercicios de hoy.

---

## Interpretabilidad

* Gráfico de dependencia parcial para detección de spam: aumento del uso del símbolo ! aumenta probabilidad de spam, mientras que con la marca HP sucede a la inversa.

![:scale 90%](./img/pd_spam.png)



---

## Librerías para R

* **gbm**: la implementación original. Soporta gráficos de dependencia parcial para interpretabilidad.

  * https://cran.r-project.org/web/packages/gbm/index.html


* **xgboost**: usada por varios ganadores de la plataforma Kaggle. ~10 veces más rápida que gbm, soporta matrices dispersas. También soporta boosting de modelos lineales.

  * https://cran.r-project.org/web/packages/xgboost/index.html
  
---

## Resumen

* Boosting como proceso de **optimización por etapas y suave**. Por ejemplo, **gradient boosting** aproxima los gradientes en cada iteración por un árbol ajustado a ellos.

* Boosting como alternativa al bagging. Mientras que boosting es un proceso que busca la **reducción del sesgo**, el bagging se emplea como **reducción de la varianza**.

* Fácil de hacer overfiting: usar conjunto de validación como **terminación temprana**.

* **Interpretabilidad decente**: importancia de variables y gráficos de dependencia parcial.


---


class: middle, center, inverse

# Random Forest

---

class: middle, center, inverse

# Colectividades

---



   




