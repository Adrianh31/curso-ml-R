---
title: "Algoritmos de Árboles y Colectividades"
subtitle: "Curso de aprendizaje automático para el INE"
author: "Víctor Gallego y Roi Naveiro"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
    
---
class: middle, center, inverse

# Intro a métodos basados en árboles


---
class: middle, center, inverse

# Boosting

---

## Aprendiendo de los errores

* Dado un dataset de entrenamiento $\lbrace (x_i, y_i) \rbrace_{i=1}^N$, ajustamos un modelo $G(x)$ y calculamos la tasa de error sobre el conjunto de entrenamiento
\begin{equation*}
err = \frac{1}{N} \sum_{i=1}^N I(y_i \neq G(x_i))
\end{equation*}

* Partiendo de que $G(x)$ es un clasificador débil, ¿cómo podemos hacer que además vaya corrigiendo sus errores?

--

* **Boosting**: iremos aplicando el algoritmo a versiones modificadas de los datos de entrenamiento, obteniendo una secuencia de clasificadores

\begin{equation*}
G_m(x),\quad m=1,\ldots,M
\end{equation*}

---

## AdaBoost.M1 (1997)

* Remuestrear los datos de entrenamiento, dando mayor pesado a las instancias mal clasificadas.

* Algoritmo

* Terminar con que AdaBoost es un modelo aditivo con una funcion de coste exponencial.

---

## Gradient Boosting

* Los árboles son ideales para combinarlos con boosting.

---

## Hiperparámetros y regularización

---

## Interpretabilidad

---


class: middle, center, inverse

# Random Forest

---

class: middle, center, inverse

# Colectividades

---



   




